The NIMBUS class trains distributed Gaussian process models for
the cross sections of sparticle production at the LHC.

To generate a datafile with the correct form use the harvest_slha_nimbus.py script in the following way

>> python harvest_slha_nimbus.py 'outfile' 'directory to search' 'file tag'

The script will write all masses and cross sections to a datafile. The indices of the masses are the PDG indices of the supersymmetric particles, with the prefix 'm'. For example, the mass of cL is denoted 'm1000004'.

The cross section indices are the two produced sparticles, with the suffix '_NLO'. For example, the cross section for gluino-gluino production is denoted '1000021_1000021_NLO'.

The PDG indices are
 gluino : 1000021
 cL     : 1000004
 sL     : 1000003
 dL     : 1000001
 uL     : 1000002
 uR     : 2000002
 dR     : 2000001
 sR     : 2000003
 cR     : 2000004

To run NIMBUS a 'configfile.dat' is needed. The file 'configfile.dat' is a textfile with the Gaussian process training configurations of the different production processes included. The configuration file can be generated and modified using make_configfile.py

>> python make_configfile.py

The configuration file, 'configfile.dat', can also be modified directly. It determines e.g. the number of experts and training points for each process.

To run NIMBUS, use the run_nimbus.py script. Initialize an instance of the nimbus class

>> nimbus2000 = nimbus('configfile.dat', 'datafile.dat')

where 'configfile.dat' is the file generated by make_configfile.py and 'datafile.py' is the file created by harvest_slha_nimbus, containing masses and cross sections for training.

Then use the 'setup()' function to apply cuts and create arrays of features and targets

>> nimbus2000.setup()

The function 'setup()' must be used before 'run()', otherwise an error occurs. The function 'run()' then loops over all production processes speficied in 'configfile.dat', and trains GP experts in parallel for each process. The number of experts is specified in 'configfile.dat'.

The experts, or models, are saved in folders according to the process index. Each process gets a folder, indexed by 'process index'_'time', where 'time' is used to avoid overwriting. All the experts of a given model are saved in the same folder, and can be loaded using

>> joblib.load(filepath)





