"""
 NIMBUS: A class for building DGP models of supersymmetric
 cross sections at the LHC.

 The class must be called in an initialize.py file, and 
 takes a data file and a configuration file as inputs.

 @author: Ingrid A V Holm
"""

import pandas as pd
import numpy as np
import sys

import sklearn
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import WhiteKernel, Matern, ConstantKernel as C
from sklearn.model_selection import train_test_split
from sklearn.externals import joblib
from joblib import Parallel, delayed

# endre til aa lese prosesser fra tekstfil
# endre index til et tall - ikke prosess
# 'hardkode' inn features
# Bruk PDG-koder for skvarker tid_1000001_1000001_2

class nimbus:
    """
     A class to train distributed Gaussian process models for strong 
     supersymmetric processes at the LHC. The class takes as input: 

         1) A datafile containing masses and cross sections for training.

         2) A configuration file with the following information for all 
            desired processes: 
             -   Type of process: 'gg' (gluino-gluino), 'qq' (squark-squark), 
                 'gq' (gluino-squark) or 'qqbar' (squark-antisquark)
    
             -   Target transformation: 'div mg' (divide the cross sections
                 by the gluino mass squared), 'mul mg' (multiply the cross
                 by the gluino mass squared) or 0 (no changes)
    
             -   Outliers: Whether to remove cross sections set to zero by
                 Prospino: 'Remove' or 'Keep'.
    
             -   Cut on cross sections: Remove cross sections below the given
                 value, either a float or 0.

             -   Experts: Number of experts to use in training the DGP
             
             -   Number of points: Number of training points per expert
    
             -   Kernel: Kernel to use in Gaussian process regression, 
                 'M' (Matern kernel) or 'RBF' (RBF kernel)
    
             -   Noise in data: Whether to estimate the noise level in the
                 data using a WhiteKernel, or setting it manually by setting
                 a float.
    """

    def __init__(self, configfile, datafile):
        """
        Initialize an instance with a datafile and configuration file.
        """
        
        print 'This initializes the class NIMBUS. Welcome!'
        
        self.df_config = pd.read_table(configfile, sep='\t')        
        self.df_data = pd.read_csv(datafile, sep=' ', skipinitialspace=True, index_col=False) # Change this later!
        self.df_data.columns = ['1.file', 'm1000021','m1000004', 'm1000003', 'm1000001', 'm1000002', 'm2000002', 'm2000001','m2000003','m2000004','1000021_1000021_NLO']# Remove this later!



        
    def setup(self):
        """
        The function that readies everything for the DGP learning and training. 
        Calls the 'data_transformation' function for every process given in the
        configuration file. 

        Returns/gives an array of features and an array of targets for every 
        process.
        """
        print "I'm starting to set up..."

        self.df_config = self.df_config.rename(columns = {'Unnamed: 0':'Parton1'})
        self.df_config = self.df_config.rename(columns = {'Unnamed: 1':'Parton2'})

        print "I call the transformation function"

        # Create empty dataframes for features and targets
        #self.features = pd.DataFrame() # MAKE A BETTER SOLUTION FOR THIS
        self.features = range(len(self.df_config.index))
        self.target = pd.DataFrame()

        for process_index in self.df_config.index:
            self.data_transformation(process_index)

        self.features = np.asarray(self.features)

            
    def run(self):
        """
        Runs the DGP training for all processes. Calls the 'DGP' function for
        every process given in the configuration file. 

        'run' must be called after 'setup' so that every process has received
        a feature and target array.
        """
        print "I train the distributed Gaussian process models for all processes"

        # Give error message if 'setup' has not initialized the feature array
        try:
            self.features
        except AttributeError:
            print "Error: Set up before running!"
            sys.exit()

        # Run over all processes in the configuration file
        for process_index in self.df_config.index:
            print "Index: ", process_index
            self.DGP(process_index)



            
    def data_transformation(self, process_index):
        """
        This function transforms the data and creates feature 
        and target arrays, that are stored in a DataFrame. Specifications
        for the transformations are in the config-file.

        Input is the index of the process, and the function returns an 
        instance of self.
        """

        target_outliers = self.df_config.loc[process_index]['Outliers']
        target_cut = self.df_config.loc[process_index]['Cut']

        # Get the partons of the process
        parton1 = self.df_config.loc[process_index]['Parton1']
        parton2 = self.df_config.loc[process_index]['Parton2']

        # Decide the process
        process_name = str(parton1)+'_'+str(parton2)+'_NLO'
        parton1_mass = 'm'+str(parton1)
        parton2_mass = 'm'+str(parton2)
        
        # Remove outliers if the config file says so
        if target_outliers == 'Remove':
            mask_outlier = self.df_data[process_name] != 0
            self.df_data_inuse = self.df_data[mask_outlier]
            #print 'DT: I removed outliers for ', process_name
        else:
            self.df_data_inuse = self.df_data

        # Introduce lower limit if the config file says so
        if target_cut > 0:
            mask_cut = self.df_data[process_name] > target_cut
            self.df_data_inuse = self.df_data_inuse[mask_cut]
            #print 'DT: I removed small cross sections for ', process_name 
        
        
        # Build the feature and target arrays
        inside_features = self.build_features(process_index)
        inside_target = self.build_target(process_index)


        # Add the features and target to the outside dataframes
        self.target = self.target.append(pd.DataFrame({process_index : inside_target}))
        # Better to store in an array? 
        self.features[process_index] = inside_features
        

        
    def build_target(self, process_index):

        # Get transformation information from config file
        transformation = self.df_config.loc[process_index]['Target']

        # Get the partons of the process
        parton1 = self.df_config.loc[process_index]['Parton1']
        parton2 = self.df_config.loc[process_index]['Parton2']

        # Decide the process
        process_name = str(parton1)+'_'+str(parton2)+'_NLO'

        target = self.df_data_inuse[process_name].values.ravel()
        mgluino = self.df_data_inuse['m1000021'].values.ravel()
        
        if transformation:
            if transformation == 'mult mg':
                print 'The cross sections are multiplied by the gluino mass squared'
                target = target*mgluino**2
                
            elif transformation == 'div mg':
                print 'The cross sections are divided by the gluino mass squared'
                target = target/mgluino**2
            #elif: something else you might want to do
            else:
                print 'Leave target as is'
                
        return target


    
        
    def build_features(self, process_index):

        squark_list = [1000004, 1000003, 1000001, 1000002,
                       2000002, 2000001, 2000003, 2000004]
        squark_mass_list = ['m1000004', 'm1000003', 'm1000001', 'm1000002',
                            'm2000002', 'm2000001', 'm2000003', 'm2000004']
        gluino = 'm1000021'
        
        # Get feature specification from config file
        parton1 = self.df_config.loc[process_index]['Parton1']
        parton2 = self.df_config.loc[process_index]['Parton2']
        production_type = self.df_config.loc[process_index]['Type']

        # Find array with mean squark mass
        m_mean = self.df_data_inuse[squark_mass_list].mean(axis=1).values.ravel()

        # Set feature index lists depending on production type
        if production_type == 'gg':
            # For gluino production use all squark masses, the gluino
            # mass and the mean squark mass
            feature_list = squark_mass_list
            feature_list.append(gluino)
            
        elif production_type == 'qq':
            # For squark pair production use the two masses, the gluino
            # mass and the mean squark mass
            if parton1 == parton2:
                feature_list = ['m'+str(parton1), gluino]
            else: 
                feature_list = ['m'+str(parton1), 'm'+str(parton2), gluino]
                
        elif production_type == 'qqbar':
            # For squark-antisquark production use the two masses, the
            # gluino mass and the mean squark mass

            if parton1 == -parton2:
                feature_list = ['m'+str(parton1), gluino]
            else:
                feature_list = ['m'+str(parton1), 'm'+str(parton2), gluino]
                
        elif production_type == 'gq':
            feature_list = ['m'+str(parton1), 'm'+str(parton2)]

        print 'The feature list for %s is %s' % (process_index, feature_list)

        # Get array of features
        features_nomean = self.df_data_inuse[feature_list].values

        # Add mean squark mass to features
        features = np.concatenate((features_nomean, m_mean.reshape(-1, 1)), axis=1)
        
        return features


    

    def DGP(self, process_index):
        """
        Function that does distributed Gaussian processes to train a model, 
        given features and target. Number of points and number of experts is 
        taken from the configuration file.

        DGP can be done in sequence for all processes by calling 'run()', or 
        in parallel by looping over DGP for the process indices from the 
        external program, e.g.
        
        >> nimbus2000 = nimbus('configfile.dat', 'datafile.dat')
        >> nimbus2000.setup()
        >> for process_index in nimbus2000.df_config.index: # MPI here
        >>         nimbus2000.DGP(process_index)
        """

        # Get features and target from dataframe
        features_inside = self.features[process_index]
        target_inside = self.target[process_index]

        # Get number of experts and points per expert from config file
        n_experts = self.df_config.loc[process_index]['Experts']
        n_points = self.df_config.loc[process_index]['Points']
        
        print 'I do distributed Gaussian processes for', self.df_config.loc[process_index]['Parton1'], self.df_config.loc[process_index]['Parton2']
        print 'using %.f experts with %.f points each' % (n_experts, n_points)

        for i in range(n_experts):
            print 'Expert number ', i+1
        
        out = Parallel(delayed(GP)(i, features[i], target[i], kernel, modelname) for i in range(n_experts))

        # In case I can solve the model inside here
        #model = 'a_full_model'        
        #self.save_model(model)


        

    def save_model(self, model):
        """
        This function stores the DGP models for each expert. 
        Each process model is placed in a folder named using a 
        time.
        """
        import time
        folder = str( int( time.time() ))+'_dgp/' 
        print 'This stores the model'

# To use joblib.Parallel the function must be defined outside the class
def GP(i, features, target, kernel, modelname):
    
    my_gp = GaussianProcessRegressor(kernel=kernel, random_state=42)
    my_gp.fit(features, target)

    
    # Save the model using joblib here, this can be changed
    j = i+1
    name = modelname+str(len(features))+'_'+str(j)
    joblib.dump(my_gp, name, compress=True)


    mybool = True # True hvis dette funket
    
    return mybool
